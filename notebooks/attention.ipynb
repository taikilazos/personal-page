{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0863c1",
   "metadata": {},
   "source": [
    "# Attention & Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811f4c0",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "- Understand the attention mechanism used in Transformer.\n",
    "- So that I can talk and explain one of the most famous paper Attention is all you need (Vaswani et al., 2017)\n",
    "- Also cover the extended version of the vanilla Transformer.\n",
    "\n",
    "### Resources\n",
    "https://arxiv.org/abs/1706.03762\n",
    "\n",
    "https://machinelearningmastery.com/what-is-attention/\n",
    "\n",
    "https://machinelearningmastery.com/the-attention-mechanism-from-scratch/\n",
    "\n",
    "https://machinelearningmastery.com/the-transformer-attention-mechanism/\n",
    "\n",
    "https://machinelearningmastery.com/the-transformer-model/\n",
    "\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-attention-and-transformer-models/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d532b",
   "metadata": {},
   "source": [
    "## What is attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a461a0f",
   "metadata": {},
   "source": [
    "Attention is roughly defined as an overall alertness of the context or ability to engage with surroundings. For example our eyesight is a great embodiment of an attention mechanism. We don't process what we see piece by piece (pixel by pixel); we focus on salient features to grasp the global meaning of what we see. So the question is: is there a way for machines to also focus on the important parts instead of processing everything?\n",
    "\n",
    "The main paper we are going to talk about is \"Attention is all you need\" (Vaswani et al., 2017) which had an encoder–decoder model architecture. It basically permits the decoder to pay more attention to important parts in an input sequence (text, pixels, anything really). The primary task the paper and its related work focused on was machine translation. The ability to translate our thoughts to break the language barrier is something people forget nowadays but it is such a game changer for many occasions. Funny how a solution for one task can be applied to many other tasks and change the world.\n",
    "\n",
    "But before explaining that paper let us review the original attention mechanism (Bahdanau et al., 2014). The problem they tried to solve was that no matter the length of the input sequence, all its information had to fit in a single vector of fixed size. This is problematic because the model often lost important details with longer sequences. Especially with older models like RNN-based ones, a common problem was losing the older context.\n",
    "\n",
    "So to solve this they introduced attention which works in the following steps:\n",
    "\n",
    "1. Alignment score\n",
    "\n",
    "$$ e_{t,i} = a(s_{t-1}, h_i) $$\n",
    "\n",
    "where <br>\n",
    "$a(\\cdot)$ = alignment score function (a small feed‑forward network) that scores how well decoder state $s_{t-1}$ matches encoder state $h_i$. <br>\n",
    "$s_{t-1}$ = the previous decoder state (what has been generated so far). <br>\n",
    "$h_i$ = the encoder hidden state at input position $i$ (a vector representing the $i$‑th input token).\n",
    "\n",
    "\n",
    "So the alignment score basically indicates how well each input word matches with what the decoder wants to generate next. For example, in machine translation, if the decoder is about to produce “apple,” higher scores will land on source tokens related to “apple.”\n",
    "\n",
    "2. Weights \n",
    "\n",
    "$$\\alpha_{t,i} = \\text{softmax}(e_{t,i})$$\n",
    "\n",
    "With this we normalize the alignment score with the softmax function over $i$ (the input positions) for each decoding step $t$. This way the attention weights range from 0 to 1 and each row sums up to 1. In human language: it tells us how much attention the model puts on each input token at this step.\n",
    "\n",
    "3. Context vector\n",
    "\n",
    "$$c_t = \\sum_{i=1}^T \\alpha_{t,i} \\, h_i$$\n",
    "\n",
    "Then like we said we want to represent the context of the input as a vector with important parts weighted by the attention scores. So we do that by taking the weighted average of all encoder hidden states. A “hidden state” is the encoder’s vector representation for each input token.\n",
    "\n",
    "In short, attention lets the decoder dynamically focus on different parts of the input at each step, instead of encoding the whole sequence in a fixed-size vector with equal representation.\n",
    "\n",
    "Now that we know the original version of attention which was the groundwork for Transformers, we will look into a more developed generalized version of the attention mechanism.\n",
    "\n",
    "The general attention mechanism works in the following steps:\n",
    "\n",
    "Why $Q$, $K$, $V$? We separate what we are looking for from how to match and what to retrieve:\n",
    "\n",
    "Q = Query, what we are currently focusing on (the “question” in a retrieving task) <br>\n",
    "K = Keys, what each input item means or represents (titles, tags in a retrieving task) <br>\n",
    "V = Values, the actual content or information stored for each input item (documents)\n",
    "\n",
    "\n",
    "1. Alignment score\n",
    "\n",
    "$$e_{q,k_i} = q \\cdot k_i$$\n",
    "\n",
    "Here, $q$ is the current query vector (e.g., a token representation) and $k_i$ is the key vector at input position $i$. This compares the query with every key.\n",
    "\n",
    "2. Weights\n",
    "\n",
    "$$\\alpha_{q,k_i} = \\text{softmax}(e_{q,k_i})$$\n",
    "\n",
    "We normalize the scores over $i$ so the weights are comparable and sum to 1. This makes them an interpretable distribution of how much to attend to each position.\n",
    "\n",
    "3. Combine values\n",
    "\n",
    "$$\\text{Attention}(q,K,V) = \\sum_i \\alpha_{q,k_i} \\, v_i$$\n",
    "\n",
    "Combine the weights with the value vectors: we take a weighted sum of the value vectors $v_i$ using the attention weights. Intuitively, each value vector is multiplied by its corresponding attention weight and then summed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb410b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.28643108 -0.72573114  1.05336353]\n",
      " [-1.03697229  0.79545304  1.76948436]\n",
      " [-1.17263791 -0.56350168  1.18539286]\n",
      " [-1.53481726 -1.91497824  0.57078711]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_seed = 25\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Let us define some inputs\n",
    "\n",
    "input_1 = np.array([1, 0, 0])\n",
    "input_2 = np.array([0, 1, 0])\n",
    "input_3 = np.array([1, 1, 0])\n",
    "input_4 = np.array([0, 0, 1])\n",
    "\n",
    "inputs = np.array([input_1, input_2, input_3, input_4]) # shape: (4, 3)\n",
    "\n",
    "# now we need to initialize the weights\n",
    "# the dimension of the input is 3 thus the dimension of the weights should be 3x3\n",
    "\n",
    "W_Q = np.random.randn(3, 3)\n",
    "W_K = np.random.randn(3, 3)\n",
    "W_V = np.random.randn(3, 3)\n",
    "\n",
    "\n",
    "# Now we can compute the query, key and value for each input (same as input dim)\n",
    "Q = inputs @ W_Q\n",
    "K = inputs @ W_K\n",
    "V = inputs @ W_V\n",
    "\n",
    "\n",
    "# scoring the query vector against corresponding key vectors (step 1)\n",
    "# Transpose it such that it matches with the corresponding query vector\n",
    "\n",
    "scores = Q @ K.T # shape: (4, 4)\n",
    "\n",
    "\n",
    "# apply the softmax function to get the attention weights\n",
    "# divide the score values by the square root of the dimensionality of the key vectors to keep the gradients stable\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    # subtract max for numerical stability\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
    "\n",
    "\n",
    "# compute the weighted sum of the value vectors\n",
    "attention_output = weights @ V\n",
    "\n",
    "print(attention_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72329a",
   "metadata": {},
   "source": [
    "## Self-Attention and Multi-Head Attention\n",
    "\n",
    "Before Transformers changed the world, RNN-based architectures like LSTMs were the standard. The \"Attention Is All You Need\" paper (Vaswani et al., 2017) argued that self-attention was the better approach, hence the name.\n",
    "\n",
    "**What is self-attention?** It's the attention mechanism applied to the same sequence to see which parts of the sequence are important. When applied to different sequences, it's called cross-attention. At its core, attention is a mapping between a query and a set of key-value pairs to an output.\n",
    "\n",
    "The Transformer architecture has two key building blocks:\n",
    "1. Scaled dot-product attention\n",
    "2. Multi-head attention\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\intercal}{\\sqrt{d_k}}\\right) V \\in \\mathbb{R}^{m \\times d_v}$$\n",
    "\n",
    "This looks similar to what we discussed before, but with an important twist. The $\\sqrt{d_k}$ scaling factor prevents vanishing gradients as the dimensionality grows. Why this specific form? As dot-product magnitudes grow roughly as $\\sqrt{d_k}$, we scale by that to stabilize training.\n",
    "\n",
    "Unlike earlier additive attention, this uses dot products ($QK^\\intercal$), which makes it computationally more efficient and easier to parallelize.\n",
    "\n",
    "**Dimensions matter here:**\n",
    "- $Q \\in \\mathbb{R}^{m \\times d_k}$ — $m$ queries, each with $d_k$ features\n",
    "- $K \\in \\mathbb{R}^{n \\times d_k}$ — $n$ keys, each with $d_k$ features  \n",
    "- $V \\in \\mathbb{R}^{n \\times d_v}$ — $n$ values, each with $d_v$ features\n",
    "- Output: $\\mathbb{R}^{m \\times d_v}$ — one value vector per query\n",
    "\n",
    "When $m = n$, we have self-attention (the sequence attends to itself). When $m \\neq n$, it's cross-attention (like encoder-decoder attention in translation).\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to look at the sequence from multiple perspectives simultaneously. Each head focuses on different relationships: one might look at semantic meaning, another at grammatical structure, another at position. It's like getting opinions from multiple experts.\n",
    "\n",
    "**Why parallelizable?** Each head runs the scaled dot-product attention independently, then we concatenate all outputs and apply a linear projection to mix them together.\n",
    "\n",
    "**In practice:** With 8 heads and $d_v = 64$ per head, each head produces $\\mathbb{R}^{m \\times 64}$. We concatenate these into $\\mathbb{R}^{m \\times 64 \\times 8}$, then project to the model dimension.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\n",
    "\n",
    "where each head is:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "and $W^O \\in \\mathbb{R}^{(h \\times d_v) \\times d_\\text{model}}$ is the output projection matrix.\n",
    "\n",
    "<img src=\"multi-head-attention.png\" width=\"50%\" alt=\"Multi-head attention diagram\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b33d8",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "<img src=\"transformer.png\" width=\"50%\" alt=\"Transformer architecture\">\n",
    "\n",
    "Now we can finally look at the Transformer architecture! In short, Transformer consists of two main parts: encoder and decoder. The left side is the encoder, it maps an input sequence to continuous representations and then passes this to the decoder. The decoder (right side) generates an output sequence, using both the encoder's output and the previously decoded output from earlier time steps.\n",
    "\n",
    "Both encoder and decoder have $N$ layers built sequentially: layer 1's output goes to layer 2, and so on. Layer $N$'s output is the final output from each part.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "First, it's important to know that Transformer has no built-in way to understand position in an input sequence, since it doesn't use recurrence or convolution. But positional understanding is crucial, so they add positional encodings before feeding data into the encoder (or decoder).\n",
    "\n",
    "The first sublayer is multi-head self-attention, which we explained earlier. This lets the model know which parts of the sequence are more important. Then it goes through a Fully Connected Feed-Forward Network (FFN), which is just two linear layers with a ReLU activation:\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(W_1 x + b_1)W_2 + b_2$$\n",
    "\n",
    "(Note: each layer has different parameters, of course.)\n",
    "\n",
    "We also see residual connections (the input is added to the output of each sublayer) to prevent the deep network from forgetting or distorting the original sequence as it goes through many layers. We also use Layer Normalization for numerical stability.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder is similar to the encoder but with an additional sublayer at the beginning. As input, it takes the output from the previous time step (what has been generated so far). Important: at the first step, it hasn't generated anything yet, so we add a special token like BOS (beginning of sentence) to handle this edge case.\n",
    "\n",
    "The first sublayer is masked multi-head attention. The mask ensures the model can only attend to previous positions—preventing it from \"looking ahead\" at future tokens during training. This makes the decoder autoregressive.\n",
    "\n",
    "Then it has multi-head attention where $Q$ comes from the decoder and $K$, $V$ come from the encoder output. This is cross-attention: the decoder queries the encoder's representation to decide what to generate next.\n",
    "\n",
    "The FC FFN adds non-linearity, enabling the model to learn complex patterns from the data.\n",
    "\n",
    "**Why $N$ layers?** Because it's simple yet powerful. More layers means more parameters and a heavier model, prone to overfitting. $N=6$ was the original choice that balanced complexity and performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b43d52af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: encoder=torch.Size([2, 10, 128]), decoder=torch.Size([2, 10, 128])\n",
      "Output shape: torch.Size([2, 10, 1000])\n",
      "Number of parameters: 1,054,696\n",
      "voila! We made a 1M parameter language model!\n"
     ]
    }
   ],
   "source": [
    "# Now we know how each component in Transformer works. Let's code it up!\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(attn_output + x)  # Residual connection\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(ff_output + x)  # Residual connection\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) \n",
    "                                    for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.masked_attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        # Masked self-attention (prevents looking ahead)\n",
    "        attn_output, _ = self.masked_attention(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(attn_output + x)\n",
    "        \n",
    "        # Cross-attention: query from decoder, key/value from encoder\n",
    "        attn_output, _ = self.attention(x, encoder_output, encoder_output)\n",
    "        x = self.norm2(attn_output + x)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(ff_output + x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) \n",
    "                                    for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, mask)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, d_vocab):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff)\n",
    "        self.projection = nn.Linear(d_model, d_vocab)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_output = self.encoder(encoder_input)\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output)\n",
    "        return self.projection(decoder_output)\n",
    "\n",
    "# Create a simple demo\n",
    "num_layers = 2  # Reduced for demo\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "d_ff = 512\n",
    "d_vocab = 1000\n",
    "\n",
    "model = Transformer(num_layers, d_model, num_heads, d_ff, d_vocab)\n",
    "\n",
    "# Example inputs (batch_size=2, seq_len=10, d_model=128)\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "encoder_input = torch.randn(batch_size, seq_len, d_model)\n",
    "decoder_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output = model(encoder_input, decoder_input)\n",
    "print(f\"Input shape: encoder={encoder_input.shape}, decoder={decoder_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"voila! We made a 1M parameter language model!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
